{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset from Hugging Face and write to UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UC_CATALOG = \"users\"\n",
    "UC_SCHEMA = \"alex_miller\"\n",
    "UC_TABLE = \"imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54311fdb-5a6e-45cb-8756-7ab59d5840d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436c6686-c1aa-42ef-b6f9-30207740e618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = ds['train'].to_pandas()\n",
    "val_dataset = ds['unsupervised'].to_pandas()\n",
    "test_dataset = ds['test'].to_pandas()\n",
    "all_dataset = pd.concat([train_dataset, val_dataset, test_dataset], ignore_index=True)\n",
    "spark_dataframe = spark.createDataFrame(all_dataset) \\\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "display(spark_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3cf61d-c8b7-499d-a8c1-5cab54764856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_dataframe.write.mode(\"overwrite\").saveAsTable(f\"{UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings using AI_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}_embeddings AS\n",
    "          SELECT\n",
    "            *,\n",
    "            AI_QUERY(\n",
    "              'databricks-gte-large-en', \n",
    "              text\n",
    "            ) AS embeddings\n",
    "          FROM {UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(f\"{UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}_embeddings\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-download-dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
