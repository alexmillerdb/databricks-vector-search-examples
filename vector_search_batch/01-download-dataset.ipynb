{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset and Create Embeddings\n",
    "\n",
    "This notebook demonstrates how to download a dataset from Hugging Face, process it, and create embeddings using Databricks AI functions. The workflow includes:\n",
    "\n",
    "1. **Dataset Download**: Load the Stanford IMDB movie review dataset from Hugging Face\n",
    "2. **Data Processing**: Combine train, test, and unsupervised datasets into a single DataFrame\n",
    "3. **Unity Catalog Storage**: Save the processed dataset to Unity Catalog\n",
    "4. **Embedding Generation**: Create vector embeddings using Databricks AI_QUERY function\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Access to Databricks workspace with Unity Catalog enabled\n",
    "- Permissions to create tables in the specified catalog and schema\n",
    "- Access to Databricks Foundation Model APIs for embedding generation\n",
    "- Internet connectivity to download from Hugging Face\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "We'll be working with the **Stanford IMDB Movie Review Dataset**:\n",
    "- **Source**: `stanfordnlp/imdb` on Hugging Face\n",
    "- **Content**: Movie reviews with sentiment labels\n",
    "- **Size**: ~100,000 movie reviews\n",
    "- **Purpose**: Text classification and sentiment analysis\n",
    "\n",
    "## Step 1: Configuration\n",
    "\n",
    "Configure your Unity Catalog destination for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UC_CATALOG = \"users\"\n",
    "UC_SCHEMA = \"alex_miller\"\n",
    "UC_TABLE = \"imdb\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Download Dataset from Hugging Face\n",
    "\n",
    "Load the Stanford IMDB dataset using the Hugging Face `datasets` library. This dataset contains movie reviews with sentiment labels and is commonly used for text classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54311fdb-5a6e-45cb-8756-7ab59d5840d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Process and Combine Dataset\n",
    "\n",
    "Process the downloaded dataset by:\n",
    "\n",
    "1. **Convert to Pandas**: Transform Hugging Face dataset splits into pandas DataFrames\n",
    "2. **Combine splits**: Merge train, test, and unsupervised datasets into a single dataset\n",
    "3. **Add unique IDs**: Generate monotonically increasing IDs for each record\n",
    "4. **Convert to Spark**: Create a Spark DataFrame for efficient processing and storage\n",
    "\n",
    "The resulting DataFrame contains:\n",
    "- **text**: Movie review text\n",
    "- **label**: Sentiment label (0 = negative, 1 = positive, or unlabeled)\n",
    "- **id**: Unique identifier for each review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436c6686-c1aa-42ef-b6f9-30207740e618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = ds['train'].to_pandas()\n",
    "val_dataset = ds['unsupervised'].to_pandas()\n",
    "test_dataset = ds['test'].to_pandas()\n",
    "all_dataset = pd.concat([train_dataset, val_dataset, test_dataset], ignore_index=True)\n",
    "spark_dataframe = spark.createDataFrame(all_dataset) \\\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "display(spark_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Save Dataset to Unity Catalog\n",
    "\n",
    "Save the processed dataset to Unity Catalog for persistent storage and easy access across your Databricks workspace. The data is stored in Delta format, providing:\n",
    "\n",
    "- **ACID transactions**: Reliable data operations\n",
    "- **Time travel**: Version history and rollback capabilities\n",
    "- **Schema enforcement**: Data quality and consistency\n",
    "- **Optimized performance**: Fast queries and analytics\n",
    "\n",
    "The table will be created at: `{UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3cf61d-c8b7-499d-a8c1-5cab54764856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_dataframe.write.mode(\"overwrite\").saveAsTable(f\"{UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Embeddings using AI_QUERY\n",
    "\n",
    "Generate vector embeddings for the movie review texts using Databricks AI_QUERY function. This step:\n",
    "\n",
    "1. **Creates a new table**: `{UC_TABLE}_embeddings` with all original columns plus embeddings\n",
    "2. **Generates embeddings**: Uses the `databricks-gte-large-en` model to create 1024-dimensional vectors\n",
    "3. **Processes all records**: Applies the embedding function to each review text\n",
    "\n",
    "### About the Embedding Model\n",
    "\n",
    "- **Model**: `databricks-gte-large-en` (General Text Embeddings)\n",
    "- **Dimensions**: 1024 (suitable for semantic similarity tasks)\n",
    "- **Use case**: Optimized for English text understanding and similarity search\n",
    "- **Performance**: Handles large text volumes efficiently\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "The embeddings table will contain:\n",
    "- All original columns (`text`, `label`, `id`)\n",
    "- New `embeddings` column with 1024-dimensional vectors\n",
    "- Ready for vector search index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}_embeddings AS\n",
    "          SELECT\n",
    "            *,\n",
    "            AI_QUERY(\n",
    "              'databricks-gte-large-en', \n",
    "              text\n",
    "            ) AS embeddings\n",
    "          FROM {UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Verify Embeddings Table\n",
    "\n",
    "Display the embeddings table to verify that the embeddings were successfully created. You should see:\n",
    "\n",
    "- Original movie review data\n",
    "- The new `embeddings` column containing 1024-dimensional vectors\n",
    "- All records processed with embeddings generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(f\"{UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}_embeddings\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "After completing this notebook, you'll have:\n",
    "\n",
    "1. **Raw dataset**: `{UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}` - Original IMDB reviews\n",
    "2. **Embeddings dataset**: `{UC_CATALOG}.{UC_SCHEMA}.{UC_TABLE}_embeddings` - Reviews with vector embeddings\n",
    "\n",
    "**Continue to**: `02-create-vector-search-index.ipynb` to create a vector search index from your embeddings.\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "### Performance Considerations\n",
    "- **Dataset size**: ~100k reviews may take 10-15 minutes to process embeddings\n",
    "- **Embedding generation**: AI_QUERY processes records in batches automatically\n",
    "- **Resource usage**: Monitor cluster resources during embedding generation\n",
    "\n",
    "### Cost Optimization\n",
    "- **Foundation Model APIs**: Embedding generation incurs costs per token processed\n",
    "- **Cluster sizing**: Use appropriately sized clusters for your dataset\n",
    "- **Batch processing**: AI_QUERY automatically optimizes batch sizes\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Common Issues:**\n",
    "- **Permission errors**: Ensure you have CREATE TABLE permissions in Unity Catalog\n",
    "- **Model access**: Verify access to Databricks Foundation Model APIs\n",
    "- **Memory issues**: For very large datasets, consider processing in chunks\n",
    "\n",
    "**Data Quality:**\n",
    "- Review text should be clean and properly formatted\n",
    "- Check for any null or empty text values before embedding generation\n",
    "- Verify embedding dimensions match your expected model output (1024 for gte-large-en)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Databricks AI_QUERY Documentation](https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html)\n",
    "- [Hugging Face Datasets Library](https://huggingface.co/docs/datasets/)\n",
    "- [Unity Catalog Documentation](https://docs.databricks.com/en/data-governance/unity-catalog/)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-download-dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
