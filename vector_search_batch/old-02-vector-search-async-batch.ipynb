{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78ecf02-ad3d-44aa-832b-7d9f8775ab63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Option 1: Async Vector Search with Configurable Concurrency using `asyncio` and `httpx`\n",
    "- Loads the Spark dataframe, selects the `query_text` (can also provide `query_vector` for embeddings), and converts to Python list\n",
    "- This will load data into memory and then runs async processes with automatic retry logic\n",
    "- Good for datasets < 1M records\n",
    "- Can use Serverless CPU compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada491db-01f1-45f7-b570-dab5f1e4eccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch httpx\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "196df7db-ea8c-48b9-88fb-a7a4a2af0e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# VS Endpoint Name\n",
    "VECTOR_SEARCH_ENDPOINT = \"abs_test_temp\"\n",
    "# Index-Name\n",
    "VECTOR_SEARCH_INDEX = \"users.alex_miller.spark_docs_vs_index\"\n",
    "\n",
    "# Embedding Dimensions\n",
    "EMBEDDING_DIMENSION = 1024\n",
    "\n",
    "# Text Dataset having embeddings\n",
    "SOURCE_DATASET = \"users.alex_miller.spark_docs_gold\"\n",
    "\n",
    "source_df = spark.table(SOURCE_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1e4647-583a-4340-9218-b82025f38664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93d7d6b6-9df6-4a34-ae26-94b4fda344db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Single REST API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d0d23c-68e3-4807-b1bc-9ad380008769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# single example text query\n",
    "single_row_example = source_df.select(\"content\", \"uuid\").limit(1).toPandas()\n",
    "query_text = single_row_example['content'][0]\n",
    "uuid = single_row_example['uuid'][0]\n",
    "\n",
    "print(\"Query Text: \", query_text)\n",
    "print(\"UUID: \", uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d57f49c-983c-417c-88ee-8cb71571c91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "WORKSPACE_URL = dbutils.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "INDEX_NAME = \"users.alex_miller.spark_docs_vs_index\"\n",
    "columns_to_include = [\"filepath\", \"content\", \"category\", \"uuid\"]\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# # Create the filters dictionary\n",
    "# filters_dict = {\"uuid\": uuid}\n",
    "\n",
    "# # Serialize to JSON string as required by the API\n",
    "# filters_json = json.dumps(filters_dict)\n",
    "\n",
    "payload = {\n",
    "    \"num_results\": 5,\n",
    "    \"query_text\": query_text,\n",
    "    \"columns\": columns_to_include,\n",
    "    \"query_type\": \"HYBRID\", # Or ANN,\n",
    "    # \"filters_json\": filters_json # Not supported in storage optimized endpoints\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{WORKSPACE_URL}/api/2.0/vector-search/indexes/{INDEX_NAME}/query\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Get column names and their indices\n",
    "columns = [col['name'] for col in data['manifest']['columns']]\n",
    "col_idx = {name: idx for idx, name in enumerate(columns)}\n",
    "\n",
    "# Prepare the result dictionary\n",
    "result_dict = {\n",
    "    \"filepath\": [],\n",
    "    \"uuid\": [],\n",
    "    \"content\": [],\n",
    "    \"score\": []\n",
    "}\n",
    "\n",
    "# Extract rows and populate the dictionary\n",
    "for row in data['result']['data_array']:\n",
    "    result_dict['filepath'].append(row[col_idx['filepath']])\n",
    "    result_dict['uuid'].append(row[col_idx['uuid']])\n",
    "    result_dict['content'].append(row[col_idx['content']])\n",
    "    result_dict['score'].append(row[col_idx['score']])\n",
    "\n",
    "# result_dict now has the desired structure\n",
    "print(result_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6160b9a8-dc06-4d01-a447-b9fb9f5ac25e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Async functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81105227-c456-4115-98b8-68eda4b65bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "from typing import List, Optional\n",
    "\n",
    "def get_config(index_name=None):\n",
    "    \"\"\"Retrieve Databricks workspace URL, API token, and index name.\"\"\"\n",
    "    WORKSPACE_URL = dbutils.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    INDEX_NAME = index_name or \"users.alex_miller.spark_docs_vs_index\"\n",
    "    return WORKSPACE_URL, TOKEN, INDEX_NAME\n",
    "\n",
    "def build_headers(token):\n",
    "    \"\"\"Build HTTP headers for the API call.\"\"\"\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "def build_payload(\n",
    "    query_text=None,\n",
    "    query_vector=None,\n",
    "    columns=None,\n",
    "    num_results=5,\n",
    "    query_type=\"HYBRID\",\n",
    "    filters_json=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Construct the payload for the vector search query.\"\"\"\n",
    "    payload = {\n",
    "        \"num_results\": num_results,\n",
    "        \"columns\": columns or [\"filepath\", \"content\", \"category\", \"uuid\"],\n",
    "        \"query_type\": query_type\n",
    "    }\n",
    "    if query_text is not None:\n",
    "        payload[\"query_text\"] = query_text\n",
    "    if query_vector is not None:\n",
    "        payload[\"query_vector\"] = query_vector\n",
    "    if filters_json is not None:\n",
    "        payload[\"filters_json\"] = filters_json\n",
    "    payload.update(kwargs)\n",
    "    return payload\n",
    "\n",
    "async def query_vector_search(\n",
    "    client, workspace_url, index_name, headers, payload,\n",
    "    max_retries=5, backoff_factor=2\n",
    "):\n",
    "    \"\"\"Async API call with retry logic for transient errors.\"\"\"\n",
    "    url = f\"{workspace_url}/api/2.0/vector-search/indexes/{index_name}/query\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = await client.post(url, headers=headers, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 429 or response.status_code >= 500:\n",
    "                wait = backoff_factor ** attempt\n",
    "                print(f\"Retrying in {wait} seconds due to status {response.status_code}...\")\n",
    "                await asyncio.sleep(wait)\n",
    "            else:\n",
    "                print(f\"Failed: {response.status_code} - {response.text}\")\n",
    "                break\n",
    "        except httpx.RequestError as e:\n",
    "            print(f\"Request error: {e}, retrying...\")\n",
    "            await asyncio.sleep(backoff_factor ** attempt)\n",
    "    return {}\n",
    "\n",
    "def parse_response(data, fields=None):\n",
    "    \"\"\"Parse API response to extract desired fields as list of row dicts.\"\"\"\n",
    "    if not data or \"result\" not in data or \"data_array\" not in data[\"result\"]:\n",
    "        return []\n",
    "    columns = [col['name'] for col in data['manifest']['columns']]\n",
    "    col_idx = {name: idx for idx, name in enumerate(columns)}\n",
    "    fields = fields or [\"filepath\", \"uuid\", \"content\", \"score\"]\n",
    "    rows = []\n",
    "    for row in data['result']['data_array']:\n",
    "        row_dict = {field: row[col_idx[field]] for field in fields if field in col_idx}\n",
    "        rows.append(row_dict)\n",
    "    return rows\n",
    "\n",
    "async def async_vector_search_batch(\n",
    "    queries: List[str],\n",
    "    index_name: Optional[str] = None,\n",
    "    columns: Optional[List[str]] = None,\n",
    "    num_results: int = 5,\n",
    "    query_type: str = \"HYBRID\",\n",
    "    filters_json: Optional[str] = None,\n",
    "    query_vector_list: Optional[List[list]] = None,\n",
    "    lookup_ids: Optional[List[str]] = None,\n",
    "    concurrency: int = 100,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Run vector search for a batch of queries asynchronously with concurrency control.\n",
    "    Optionally attaches lookup_content and lookup_id to each result row.\n",
    "    \"\"\"\n",
    "    workspace_url, token, index_name = get_config(index_name)\n",
    "    headers = build_headers(token)\n",
    "    columns = columns or [\"filepath\", \"content\", \"category\", \"uuid\"]\n",
    "\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def sem_task(query, idx):\n",
    "        async with semaphore:\n",
    "            payload = build_payload(\n",
    "                query_text=query if query_vector_list is None else None,\n",
    "                query_vector=None if query_vector_list is None else query_vector_list[idx],\n",
    "                columns=columns,\n",
    "                num_results=num_results,\n",
    "                query_type=query_type,\n",
    "                filters_json=filters_json,\n",
    "                **kwargs\n",
    "            )\n",
    "            async with httpx.AsyncClient(timeout=30) as client:\n",
    "                response = await query_vector_search(client, workspace_url, index_name, headers, payload)\n",
    "            parsed_results = parse_response(response, fields=columns + [\"score\"])\n",
    "            # Attach lookup_content and lookup_id if provided\n",
    "            for row in parsed_results:\n",
    "                row[\"lookup_content\"] = query\n",
    "                if lookup_ids is not None:\n",
    "                    row[\"lookup_id\"] = lookup_ids[idx]\n",
    "            return parsed_results\n",
    "\n",
    "    tasks = [sem_task(query, i) for i, query in enumerate(queries)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    all_rows = [row for batch in results for row in batch]\n",
    "    return all_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e1be102-318b-4cdd-b87d-2b2320ff9235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run Async functions with 100 concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2567a54-96d7-4dfc-9261-db3ae504e198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract query texts from the DataFrame\n",
    "query_texts = source_df.select(\"content\").limit(1000).toPandas()[\"content\"].tolist()\n",
    "lookup_ids = source_df.select(\"uuid\").limit(1000).toPandas()[\"uuid\"].tolist()\n",
    "print(f\"Prepared {len(query_texts)} queries for vector search.\")\n",
    "\n",
    "# Run async batch search with controlled concurrency (e.g., 100)\n",
    "all_rows = await async_vector_search_batch(\n",
    "    queries=query_texts,\n",
    "    lookup_ids=lookup_ids,\n",
    "    index_name=\"users.alex_miller.spark_docs_vs_index\",\n",
    "    columns=[\"filepath\", \"content\", \"category\", \"uuid\"],\n",
    "    num_results=5,\n",
    "    query_type=\"HYBRID\",\n",
    "    concurrency=100\n",
    ")\n",
    "\n",
    "# Create and display the Spark DataFrame\n",
    "sdf = spark.createDataFrame(all_rows)\n",
    "print(f\"Spark DataFrame created with {sdf.count()} rows.\")\n",
    "display(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba9d408-4db3-4c87-9f23-97ddfe29ae7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Option 2: Use Ray (Ray Data and Ray Core) to load data into Ray dataset and run async processing\n",
    "- Cluster config below but make sure to update `setup_ray_cluster` configurations to correct setting ([documentation](https://docs.databricks.com/aws/en/machine-learning/ray/scale-ray))\n",
    "- Utilizes similar processing logic as above but uses Ray Data and Ray Core to orchestrate the parallel processing\n",
    "- Good for large datasets that don't fit into memory (why we use Ray Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be266db2-a1f0-4f9b-b116-e9ce573c42d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch httpx\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd52151-91e5-46d8-947a-f5ab5a8020fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "import ray\n",
    "\n",
    "# Configure Ray cluster (adjust worker nodes and CPUs as needed)\n",
    "setup_ray_cluster(\n",
    "  min_worker_nodes=1,     # Minimum number of worker nodes on Spark cluster\n",
    "  max_worker_nodes=5,     # Maximum number of worker nodes on Spark cluster\n",
    "  num_cpus_per_node=16,   # Number of CPUs per worker node\n",
    "  num_cpus_head_node=8,   # Number of CPUs on head node (give Spark some CPUs)\n",
    "  num_gpus_head_node=0,\n",
    "  num_gpus_worker_node=0\n",
    "  )\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89156365-59c0-4f5b-8d12-9bf076462f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Prep and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850083e9-91a4-466e-b6cc-8ffc8cac4dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray.data\n",
    "\n",
    "# VS Endpoint Name\n",
    "VECTOR_SEARCH_ENDPOINT = \"abs_test_temp\"\n",
    "# Index-Name\n",
    "VECTOR_SEARCH_INDEX = \"users.alex_miller.spark_docs_vs_index\"\n",
    "# UC variables\n",
    "CATALOG = \"users\"\n",
    "SCHEMA = \"alex_miller\"\n",
    "VOLUME_NAME = \"ray\"\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME_NAME}\")\n",
    "\n",
    "# Embedding Dimensions\n",
    "EMBEDDING_DIMENSION = 1024\n",
    "\n",
    "# Text Dataset having embeddings\n",
    "SOURCE_DATASET = f\"{CATALOG}.{SCHEMA}.spark_docs_gold\"\n",
    "UC_VOLUME_FOR_RAY = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}/temp\"   # temp directory in UC to store Ray dataset creation\n",
    "VECTOR_COLUMN = \"content\" # use embeddings for vector_query\n",
    "VECTOR_ID = \"uuid\"\n",
    "\n",
    "# Convert a Spark DataFrame to a Ray Dataset\n",
    "spark_df = spark.read.table(SOURCE_DATASET).select([VECTOR_COLUMN, VECTOR_ID]).limit(1000)  # Or use your DataFrame source\n",
    "ray_ds = ray.data.from_spark(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e007ab18-fecf-4cdc-a843-599586bda373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray_ds.take_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a655f2c-70ab-45bb-97ea-eca915896fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import httpx\n",
    "import json\n",
    "\n",
    "@ray.remote\n",
    "def vector_search_task(query, workspace_url, index_name, token, columns, num_results, query_type, filters_json):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"num_results\": num_results,\n",
    "        \"columns\": columns,\n",
    "        \"query_type\": query_type,\n",
    "        \"query_text\": query,\n",
    "    }\n",
    "    if filters_json:\n",
    "        payload[\"filters_json\"] = filters_json\n",
    "\n",
    "    url = f\"{workspace_url}/api/2.0/vector-search/indexes/{index_name}/query\"\n",
    "    for attempt in range(1, 6):\n",
    "        try:\n",
    "            with httpx.Client(timeout=30) as client:\n",
    "                response = client.post(url, headers=headers, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                # Parse result into a row dict\n",
    "                columns_list = [col['name'] for col in data['manifest']['columns']]\n",
    "                col_idx = {name: idx for idx, name in enumerate(columns_list)}\n",
    "                rows = []\n",
    "                for row in data['result']['data_array']:\n",
    "                    row_dict = {field: row[col_idx[field]] for field in columns + [\"score\"] if field in col_idx}\n",
    "                    rows.append(row_dict)\n",
    "                return rows\n",
    "            elif response.status_code == 429 or response.status_code >= 500:\n",
    "                import time\n",
    "                wait = 2 ** attempt\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            import time\n",
    "            time.sleep(2 ** attempt)\n",
    "    return []\n",
    "\n",
    "def flatten_results(results, columns, lookup_values, lookup_ids):\n",
    "    \"\"\"\n",
    "    Flattens nested results, extracts specified columns plus 'score',\n",
    "    and attaches the original lookup value and ID for each query.\n",
    "    \"\"\"\n",
    "    if 'score' not in columns:\n",
    "        columns = columns + ['score']\n",
    "    all_rows = []\n",
    "    for lookup_value, lookup_id, result_list in zip(lookup_values, lookup_ids, results):\n",
    "        for result in result_list:\n",
    "            row = {col: result.get(col) for col in columns}\n",
    "            row[\"lookup_content\"] = lookup_value\n",
    "            row[\"lookup_id\"] = lookup_id\n",
    "            all_rows.append(row)\n",
    "    return all_rows\n",
    "\n",
    "def ray_vector_search_batch(ray_ds, workspace_url, index_name, token, columns, num_results, query_type, filters_json, batch_size=50):\n",
    "    def process_batch(batch):\n",
    "        # batch is a dict of arrays: batch[VECTOR_COLUMN], batch[VECTOR_ID]\n",
    "        contents = batch[VECTOR_COLUMN]\n",
    "        ids = batch[VECTOR_ID]\n",
    "        results = ray.get([\n",
    "            vector_search_task.remote(\n",
    "                content, workspace_url, index_name, token, columns, num_results, query_type, filters_json\n",
    "            ) for content in contents\n",
    "        ])\n",
    "        all_rows = flatten_results(results, columns, contents, ids)\n",
    "        output_columns = columns + ([\"score\"] if \"score\" not in columns else []) + [\"lookup_content\", \"lookup_id\"]\n",
    "        output_dict = {col: [row.get(col) for row in all_rows] for col in output_columns}\n",
    "        return output_dict\n",
    "\n",
    "    results = ray_ds.map_batches(process_batch, batch_size=batch_size)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6a1cde-ee73-4264-aa90-2a24221a68b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "WORKSPACE_URL = dbutils.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "INDEX_NAME = \"users.alex_miller.spark_docs_vs_index\"\n",
    "COLUMNS = [\"filepath\", \"content\", \"category\", \"uuid\"]\n",
    "NUM_RESULTS = 5\n",
    "QUERY_TYPE = \"HYBRID\"\n",
    "FILTERS_JSON = None  # Or your filter as a JSON string\n",
    "\n",
    "# Run distributed vector search and flatten results\n",
    "all_rows = ray_vector_search_batch(\n",
    "    ray_ds=ray_ds,\n",
    "    workspace_url=WORKSPACE_URL,\n",
    "    index_name=INDEX_NAME,\n",
    "    token=TOKEN,\n",
    "    columns=COLUMNS,\n",
    "    num_results=NUM_RESULTS,\n",
    "    query_type=QUERY_TYPE,\n",
    "    filters_json=FILTERS_JSON,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "# write Ray Data to Spark\n",
    "import os\n",
    "dbutils.fs.mkdirs(UC_VOLUME_FOR_RAY)\n",
    "os.environ['RAY_UC_VOLUMES_FUSE_TEMP_DIR'] = UC_VOLUME_FOR_RAY\n",
    "_ = ray.data.Dataset.write_databricks_table(\n",
    "  ray_dataset=all_rows,\n",
    "  name=\"users.alex_miller.spark_docs_vs_batch_results\",\n",
    "  mode=\"overwrite\"    # or append\n",
    ")\n",
    "\n",
    "shutdown_ray_cluster()\n",
    "ray.shutdown()\n",
    "\n",
    "# Read and display a Spark DataFrame\n",
    "sdf = spark.read.table(\"users.alex_miller.spark_docs_vs_batch_results\")\n",
    "display(sdf)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-vector-search-async-batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
